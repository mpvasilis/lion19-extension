# Critical Fix: Mock Constraints Not Being Used in Experiments

## Problem Summary

The experiment shows that **Phase 1 is generating implied AllDifferent constraints** instead of using the carefully designed mock constraints from the benchmark files. This causes:

1. **UNSAT during Phase 2** - Can't violate subsets of global AllDifferent
2. **False acceptance** - System accepts spurious constraints as "likely correct or implied"
3. **Invalid experimental results** - Not actually testing over-fitting detection

## Root Causes

### Issue 1: Phase 1 Generates Random AllDifferent Subsets

**Location:** `phase1_passive_learning.py:440-473`

```python
# Random subset of variables
var_subset = random.sample(var_list, scope_size)

# Check if this subset is already in target
var_names = tuple(sorted([v.name for v in var_subset]))
if var_names in target_strs:
    continue  # ❌ Only checks EXACT matches, not subsets!
```

**Problem:** When target has `AllDifferent(all_30_vars)`, ANY random subset will:
- Satisfy AllDifferent in examples (they're part of the global)
- Pass the "not in target" check (different subset)
- Be IMPLIED by the global AllDifferent

**Impact:** Generates implied constraints that cause UNSAT in Phase 2

### Issue 2: Mock Constraints Not Returned by Benchmarks

**Location:** `benchmarks_global/exam_timetabling_variants.py`, etc.

```python
mock_constraints = []
# ... add mocks to model ...
return instance, oracle  # ❌ Mock constraints not returned!
```

**Problem:** Benchmarks create mock constraints for example generation but don't expose them to Phase 1

**Impact:** Phase 1 can't use the designed mocks, falls back to random generation

### Issue 3: Phase 1 Doesn't Accept Mock Constraints

**Location:** `phase1_passive_learning.py:600`

```python
def run_phase1(benchmark_name, output_dir='phase1_output', num_examples=5, num_overfitted=4):
    instance, oracle = construct_instance(benchmark_name)
    # ❌ No way to receive or use mock_constraints parameter
```

**Problem:** Even if benchmarks returned mocks, Phase 1 has no way to use them

**Impact:** Pipeline doesn't support designed overfitted constraints

## What The User's Experiment Shows

```
[UNSAT] Cannot generate violation query for remaining 5 constraints
[DECISION] Accepting remaining constraints as likely correct or implied
  [ACCEPT] alldifferent(var[1,3],var[3,1],var[0,3],var[1,1],var[3,4]) (P=0.706)
  [ACCEPT] alldifferent(var[0,3],var[3,3],var[0,2],var[3,2]) (P=0.706)
  ...
[ERROR] Spurious constraints: 5
```

These 5 "overfitted" constraints are **random subsets** generated by Phase 1, not the designed mocks!

They're **implied** by the global AllDifferent, causing UNSAT.

## Fixes Applied

### Fix 1: Prevent Subset Generation ✅

**File:** `phase1_passive_learning.py`

```python
# Check if this subset is implied by any target constraint (subset check)
var_names_set = set(var_names)
is_subset_of_target = False
for target_set in target_sets:
    if var_names_set.issubset(target_set):
        is_subset_of_target = True
        break

if is_subset_of_target:
    continue  # Skip subsets that would be implied
```

**Status:** ✅ IMPLEMENTED
**Impact:** Prevents generating AllDifferent subsets of target constraints

### Fix 2: Return Mock Constraints from Benchmarks ✅

**Files:** 
- `benchmarks_global/exam_timetabling.py`
- `benchmarks_global/exam_timetabling_variants.py`

```python
return instance, oracle, mock_constraints  # ✅ Now returns mocks
```

**Status:** ✅ IMPLEMENTED
**Impact:** Makes designed mock constraints accessible

### Fix 3: Use Mock Constraints in Phase 1 ⏳

**File:** `phase1_passive_learning.py`

**Needed Changes:**

1. Update `construct_instance()` to handle optional third return value:
```python
result = construct_examtt_variant1(...)
if len(result) == 3:
    instance, oracle, mock_constraints = result
else:
    instance, oracle = result
    mock_constraints = []
```

2. Update `run_phase1()` to use mocks if available:
```python
result = construct_instance(benchmark_name)
if len(result) == 3:
    instance, oracle, mock_constraints = result
    # Use mock_constraints directly instead of generate_overfitted_alldifferent()
else:
    instance, oracle = result
    mock_constraints = []
    # Fall back to random generation
```

**Status:** ⏳ IN PROGRESS
**Impact:** Phase 1 will use designed mocks instead of random generation

## What This Means for Experiments

### Before Fix
- Phase 1 generates random AllDifferent subsets
- These are IMPLIED by global AllDifferent
- Phase 2 gets UNSAT and accepts them
- **Result:** Invalid test of over-fitting detection

### After Fix
- Phase 1 uses designed mock constraints:
  - Ordering constraints (`var[i] < var[i+1]`)
  - Exact count constraints (`Count(...) == 2`)
  - Sum constraints (`sum(...) <= 100`)
  - Cross-semester day constraints
- These are NOT implied by true constraints
- Phase 2 can generate violation queries
- **Result:** Valid test of over-fitting detection

## Next Steps

1. ✅ Fix Phase 1 subset check
2. ✅ Make benchmarks return mock constraints  
3. ⏳ Update Phase 1 to handle and use mock constraints
4. ⏳ Regenerate Phase 1 pickle files with designed mocks
5. ⏳ Rerun Phase 2 experiments

## Expected Outcome

Phase 2 should be able to:
- Generate queries violating the mock constraints
- Get oracle feedback
- Probabilistically reject the mocks
- Learn 100% correct model (7 true, 0 spurious)

